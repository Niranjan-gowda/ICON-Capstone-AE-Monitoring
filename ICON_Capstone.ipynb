{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**The Code was developed and tested in Google Collab**\n",
        "\n",
        "The code below showcases the following:\n",
        "\n",
        "\n",
        "1.   Exraction of content from pre-pasted link of aspirin data from FDA website in an excel sheet, the content will be saved and can be downloaded as word and CSV file.\n",
        "2.   Adverse Event extraction from the content based on predefined lexicon along with brand names.\n",
        "\n",
        "\n",
        "3.   Filtering the adverse event extraction for considering only negative events using manual filtering method and sentiment analysis.\n",
        "2.   BIOPORTAL Ontology mapping.\n",
        "\n",
        "\n",
        "5.   UI presentation using streamlit.\n",
        "2.   Hosting the web server on ngork.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kcPISVdzPtpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 python-docx PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7Dc-fSpV4Sd",
        "outputId": "d8e86488-f1bd-43b5-fa5f-42098f01fd53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.7.9)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.1)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.26.3 python-docx-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import requests\n",
        "import mimetypes\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from docx import Document\n",
        "from urllib.parse import urlparse\n",
        "import fitz  # PyMuPDF\n",
        "from io import BytesIO\n",
        "from docx import Document as DocxReader\n",
        "\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    return re.sub(r'[^\\x09\\x0A\\x0D\\x20-\\x7E\\u00A0-\\uFFFF]', '', text)\n",
        "\n",
        "#Load URLs\n",
        "df = pd.read_excel(\"/content/Aspirin_trail.xlsx\")\n",
        "df.columns = df.columns.str.strip()\n",
        "urls = df[\"Links\"].dropna().tolist()\n",
        "\n",
        "doc = Document()\n",
        "doc.add_heading(\"Aspirin FDA Text Extracts\", level=0)\n",
        "scraped_data = []\n",
        "\n",
        "#Webpage content extraction\n",
        "def extract_fda_text(soup):\n",
        "    content = []\n",
        "    for container in soup.select(\"div.field-item, article, div[role='main']\"):\n",
        "        text = container.get_text(separator=' ', strip=True)\n",
        "        if text:\n",
        "            content.append(text)\n",
        "    if not content:\n",
        "        paragraphs = soup.find_all(\"p\")\n",
        "        content = [p.get_text(strip=True) for p in paragraphs]\n",
        "    return \"\\n\".join(content)\n",
        "\n",
        "#File content extraction\n",
        "def extract_from_pdf(content_bytes):\n",
        "    text = \"\"\n",
        "    with fitz.open(stream=content_bytes, filetype=\"pdf\") as pdf:\n",
        "        for page in pdf:\n",
        "            text += page.get_text()\n",
        "    return text\n",
        "\n",
        "def extract_from_docx(content_bytes):\n",
        "    file_stream = BytesIO(content_bytes)\n",
        "    docx_file = DocxReader(file_stream)\n",
        "    return \"\\n\".join([para.text for para in docx_file.paragraphs])\n",
        "\n",
        "def extract_from_txt(content_bytes):\n",
        "    return content_bytes.decode(\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "def handle_download(url, content_type, content_bytes):\n",
        "    if 'pdf' in content_type:\n",
        "        return extract_from_pdf(content_bytes), \"PDF Document\"\n",
        "    elif 'word' in content_type or 'docx' in url:\n",
        "        return extract_from_docx(content_bytes), \"Word Document\"\n",
        "    elif 'text' in content_type or url.endswith(\".txt\"):\n",
        "        return extract_from_txt(content_bytes), \"Text File\"\n",
        "    else:\n",
        "        return \"[SKIPPED] Unsupported file type\", \"Unknown\"\n",
        "\n",
        "#URL handler\n",
        "def scrape_url(url):\n",
        "    try:\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        response = requests.get(url, headers=headers, timeout=15)\n",
        "        if response.status_code != 200:\n",
        "            return f\"[ERROR] Status code {response.status_code}\", \"No Title\"\n",
        "\n",
        "        content_type = response.headers.get(\"Content-Type\", \"\").lower()\n",
        "        if \"html\" in content_type:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            title_tag = soup.find(\"title\")\n",
        "            title = title_tag.get_text(strip=True) if title_tag else \"No Title\"\n",
        "            body = extract_fda_text(soup)\n",
        "            return body, title\n",
        "        else:\n",
        "            body, filetype = handle_download(url, content_type, response.content)\n",
        "            return body, filetype\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"[EXCEPTION] {str(e)}\", \"Error\"\n",
        "\n",
        "#Main loop\n",
        "for i, url in enumerate(urls):\n",
        "    print(f\"Processing {i+1}/{len(urls)}: {url}\")\n",
        "    body_text, page_title = scrape_url(url)\n",
        "\n",
        "    if body_text.startswith(\"[SKIPPED]\") or not body_text.strip():\n",
        "        print(f\"Skipping unsupported or empty content: {url}\")\n",
        "        continue\n",
        "\n",
        "    scraped_data.append({\n",
        "        \"URL\": url,\n",
        "        \"Title\": clean_text(page_title),\n",
        "        \"Text\": clean_text(body_text)\n",
        "    })\n",
        "\n",
        "    doc.add_heading(clean_text(page_title), level=1)\n",
        "    doc.add_paragraph(clean_text(url))\n",
        "    doc.add_paragraph(clean_text(body_text))\n",
        "\n",
        "    time.sleep(1)\n",
        "\n",
        "# Save outputs\n",
        "scraped_df = pd.DataFrame(scraped_data)\n",
        "scraped_df.to_csv(\"aspirin_scraped_texts.csv\", index=False)\n",
        "doc.save(\"aspirin_scraped_texts.docx\")\n",
        "print(\"Web + File scraping completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYGFTvNqV84n",
        "outputId": "8016b662-ff9d-4518-baa1-ff50e8c90782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 1/101: https://medlineplus.gov/druginfo/meds/a682878.html\n",
            "Processing 2/101: https://www.fda.gov/consumers/consumer-updates/warning-aspirin-containing-antacid-medicines-can-cause-bleeding\n",
            "Processing 3/101: https://www.fda.gov/drugs/understanding-over-counter-medicines/safe-use-aspirin\n",
            "Processing 4/101: https://www.fda.gov/drugs/safe-use-aspirin/aspirin-questions-and-answers\n",
            "Processing 5/101: https://www.fda.gov/drugs/drug-safety-and-availability/fda-drug-safety-communication-fda-warns-about-serious-bleeding-risk-over-counter-antacid-products\n",
            "Processing 6/101: https://www.fda.gov/drugs/postmarket-drug-safety-information-patients-and-providers/information-about-taking-ibuprofen-and-aspirin-together\n",
            "Processing 7/101: https://www.fda.gov/animal-veterinary/product-safety-information/dear-veterinarian-letter-regarding-use-aspirin-products-lactating-dairy-cattle\n",
            "Processing 8/101: https://www.fda.gov/consumers/articulos-para-el-consumidor-en-espanol/advertencia-los-antiacidos-que-contienen-aspirina-pueden-causar-sangrado\n",
            "Processing 9/101: https://www.fda.gov/drugs/safe-use-aspirin/aspirin-reducing-your-risk-heart-attack-and-stroke-know-facts\n",
            "Processing 10/101: https://www.fda.gov/consumers/articulos-para-el-consumidor-en-espanol/puede-una-aspirina-al-dia-prevenir-un-ataque-cardiaco\n",
            "Processing 11/101: https://www.fda.gov/drugs/safe-use-aspirin/using-aspirin-lower-your-risk-heart-attack-or-stroke-what-you-should-know\n",
            "Processing 12/101: https://www.fda.gov/drugs/drug-safety-and-availability/fda-advierte-acerca-del-riesgo-de-sangrado-grave-en-antiacidos-de-venta-libre-que-contienen-aspirina\n",
            "Processing 13/101: https://www.fda.gov/drugs/drug-safety-and-availability/la-fda-refuerza-la-advertencia-que-los-medicamentos-sin-aspirina-antinflamatorios-no-esteroides\n",
            "Processing 14/101: https://www.fda.gov/drugs/drug-safety-and-availability/fda-drug-safety-communication-fda-strengthens-warning-non-aspirin-nonsteroidal-anti-inflammatory\n",
            "Processing 15/101: https://www.fda.gov/safety/recalls-market-withdrawals-safety-alerts/aurobindo-pharma-usa-inc-behalf-aurohealth-issues-voluntary-nationwide-recall-one-1-lot-healthy\n",
            "Processing 16/101: https://www.fda.gov/regulatory-information/search-fda-guidance-documents/recommended-statement-over-counter-aspirin-containing-drug-products-labeled-cardiovascular-related\n",
            "Processing 17/101: https://www.fda.gov/drugs/understanding-over-counter-medicines/educational-resources-understanding-over-counter-medicine\n",
            "Processing 18/101: https://www.fda.gov/media/102479/download\n",
            "Processing 19/101: https://www.fda.gov/media/76636/download\n",
            "Processing 20/101: https://www.fda.gov/media/98448/download\n",
            "Processing 21/101: https://www.fda.gov/media/98533/download\n",
            "Processing 22/101: https://www.fda.gov/media/178925/download\n",
            "Processing 23/101: https://www.fda.gov/media/92768/download\n",
            "Processing 24/101: https://www.fda.gov/media/75681/download\n",
            "Processing 25/101: https://www.fda.gov/drugs/drug-safety-and-availability/fda-drug-safety-communication-fda-review-finds-long-term-treatment-blood-thinning-medicine-plavix\n",
            "Processing 26/101: https://www.fda.gov/drugs/drug-safety-and-availability/evaluacion-de-la-fda-determina-que-el-tratamiento-prolongado-con-el-medicamento-anticoagulante\n",
            "Processing 27/101: https://www.fda.gov/consumers/free-publications-women/salud-cardiaca-heart-health\n",
            "Processing 28/101: https://www.fda.gov/consumers/womens-health-topics/heart-health-women\n",
            "Processing 29/101: https://www.fda.gov/inspections-compliance-enforcement-and-criminal-investigations/warning-letters/health-pharma-usa-llc-588155-12182019\n",
            "Processing 30/101: https://www.fda.gov/drugs/enforcement-activities-fda/otc-warning-letters\n",
            "Processing 31/101: https://www.fda.gov/drugs/unapproved-drugs/ndc-number-and-proprietary-name-approved-and-marketed-codeine-and-dihydrocodeine-containing-products\n",
            "Processing 32/101: https://www.fda.gov/inspections-compliance-enforcement-and-criminal-investigations/warning-letters/applied-biological-laboratories-inc-627062-03242022\n",
            "Processing 33/101: https://www.fda.gov/drugs/historical-status-otc-rulemakings/rulemaking-history-general-labeling-requirements-otc-drug-products\n",
            "Processing 34/101: https://www.fda.gov/drugs/enforcement-activities-fda/over-counter-otc-drugs-branch\n",
            "Processing 35/101: https://www.fda.gov/drugs/drug-safety-and-availability/fda-recommends-avoiding-use-nsaids-pregnancy-20-weeks-or-later-because-they-can-result-low-amniotic\n",
            "Processing 36/101: https://www.fda.gov/drugs/postmarket-drug-safety-information-patients-and-providers/ibuprofen-drug-facts-label\n",
            "Processing 37/101: https://www.fda.gov/drugs/understanding-over-counter-medicines/over-counter-otc-heartburn-treatment\n",
            "Processing 38/101: https://www.fda.gov/consumers/consumer-updates/drugs\n",
            "Processing 39/101: https://www.fda.gov/drugs/fdas-adverse-event-reporting-system-faers/january-march-2023-potential-signals-serious-risksnew-safety-information-identified-fda-adverse\n",
            "Processing 40/101: https://www.fda.gov/drugs/drug-safety-and-availability/comunicado-de-la-fda-sobre-la-seguridad-de-los-medicamentos-la-fda-ha-reevaluado-los-posibles\n",
            "Processing 41/101: https://www.fda.gov/drugs/historical-status-otc-rulemakings/rulemaking-history-otc-internal-analgesic-drug-products\n",
            "Processing 42/101: https://www.fda.gov/news-events/press-announcements/la-fda-advierte-que-el-uso-de-un-tipo-de-medicamento-para-el-dolor-y-la-fiebre-en-la-segunda-mitad\n",
            "Processing 43/101: https://www.fda.gov/drugs/drug-safety-and-availability/la-fda-recomienda-evitar-el-uso-de-medicamentos-aine-durante-el-embarazo-y-partir-de-las-20-semanas\n",
            "Processing 44/101: https://www.fda.gov/consumers/knowledge-and-news-women-owh-blog/women-and-heart-health\n",
            "Processing 45/101: https://www.fda.gov/drugs/drug-safety-and-availability/fda-drug-safety-communication-fda-reviews-long-term-antiplatelet-therapy-preliminary-trial-data\n",
            "Processing 46/101: https://www.fda.gov/regulatory-information/electronic-reading-room/cber-notice-initiation-disqualification-proceeding-and-opportunity-explain-nidpoe-date-issued-0\n",
            "Processing 47/101: https://www.fda.gov/drugs/drug-safety-and-availability/fda-drug-safety-communication-fda-has-reviewed-possible-risks-pain-medicine-use-during-pregnancy\n",
            "Processing 48/101: https://www.fda.gov/drugs/human-drug-compounding/questions-and-answers-compounded-oral-suspension-medications-pain-and-fever\n",
            "Processing 49/101: https://www.fda.gov/regulatory-information/electronic-reading-room/faxon-david-p-md-notice-initiation-disqualification-proceedings-and-opportunity-explain-nidpoe\n",
            "Processing 50/101: https://www.fda.gov/consumers/articulos-para-el-consumidor-en-espanol/cancer-colorrectal-lo-que-debe-saber-sobre-las-pruebas-de-deteccion\n",
            "Processing 51/101: https://www.fda.gov/drugs/fda-drug-safety-podcasts/fda-recommends-avoiding-use-nsaids-pregnancy-20-weeks-or-later-because-they-can-result-low-amniotic\n",
            "Processing 52/101: https://www.fda.gov/inspections-compliance-enforcement-and-criminal-investigations/warning-letters/fibromyalgia-treatment-group-614183-ftg-llc\n",
            "Processing 53/101: https://www.fda.gov/drugs/fdas-adverse-event-reporting-system-faers/october-december-2017-potential-signals-serious-risksnew-safety-information-identified-fda-adverse\n",
            "Processing 54/101: https://www.fda.gov/medical-devices/recently-approved-devices/biofreedom-drug-coated-coronary-stent-dcs-system-p190020\n",
            "Processing 55/101: https://www.fda.gov/drugs/news-events-human-drugs/fda-approves-drug-treat-help-prevent-types-blood-clots-certain-pediatric-populations\n",
            "Processing 56/101: https://www.fda.gov/fda-en-espanol/articulos-para-el-consumidor/medicamentos\n",
            "Processing 57/101: https://www.fda.gov/safety/medical-product-safety-information/nonsteroidal-anti-inflammatory-drugs-nsaids-drug-safety-communication-avoid-use-nsaids-pregnancy-20\n",
            "Processing 58/101: https://www.fda.gov/consumers/consumer-updates/tips-women-prevent-heart-disease\n",
            "Processing 59/101: https://www.fda.gov/consumers/consumer-updates/tips-women-prevent-heart-disease\n",
            "Processing 60/101: https://www.fda.gov/vaccines-blood-biologics/consumers-biologics/vaccines-children-guide-parents-and-caregivers\n",
            "Processing 61/101: https://www.fda.gov/drugs/drug-safety-and-availability/fda-drug-safety-communication-updated-recommendations-decrease-risk-spinal-column-bleeding-and\n",
            "Processing 62/101: https://www.fda.gov/drugs/drug-safety-and-availability/comunicado-de-la-fda-sobre-la-seguridad-de-los-medicamentos-recomendaciones-actualizadas-para\n",
            "Processing 63/101: https://www.fda.gov/drugs/data-standards-manual-monographs/data-standards-manual-monographs-package-type\n",
            "Processing 64/101: https://www.fda.gov/drugs/drug-safety-and-availability/fda-drug-safety-communication-fda-warns-about-prescribing-and-dispensing-errors-resulting-brand-name\n",
            "Processing 65/101: https://www.fda.gov/drugs/postmarket-drug-safety-information-patients-and-providers/codeine-information\n",
            "Processing 66/101: https://www.fda.gov/drugs/postmarket-drug-safety-information-patients-and-providers/lovenox-enoxaparin-information\n",
            "Processing 67/101: https://www.fda.gov/drugs/postmarket-drug-safety-information-patients-and-providers/nonsteroidal-anti-inflammatory-drugs-nsaids\n",
            "Processing 68/101: https://www.fda.gov/drugs/drug-safety-and-availability/fda-drug-safety-communication-fda-warns-possible-harm-exceeding-recommended-dose-over-counter-sodium\n",
            "Processing 69/101: https://www.fda.gov/drugs/drug-safety-and-availability/la-fda-advierte-sobre-los-errores-de-prescripcion-y-despacho-que-resultan-de-la-confusion-del-nombre\n",
            "Processing 70/101: https://www.fda.gov/regulatory-information/search-fda-guidance-documents/cpg-sec-430100-unit-dose-labeling-solid-and-liquid-oral-dosage-forms\n",
            "Processing 71/101: https://www.fda.gov/drugs/buying-using-medicine-safely/understanding-over-counter-medicines\n",
            "Processing 72/101: https://www.fda.gov/animal-veterinary/safety-health/frequently-asked-questions-about-animal-drugs\n",
            "Processing 73/101: https://www.fda.gov/drugs/medication-health-fraud/public-notification-xcel-contains-hidden-drug-ingredient\n",
            "Processing 74/101: https://www.fda.gov/drugs/medication-health-fraud/public-notification-botanical-slimming-red-contains-hidden-drug-ingredient\n",
            "Processing 75/101: https://www.fda.gov/drugs/medication-health-fraud/public-notification-oxy-elite-pro-super-thermogenic-contains-hidden-drug-ingredient\n",
            "Processing 76/101: https://www.fda.gov/drugs/medication-health-fraud/public-notification-toxin-discharged-tea-contains-hidden-drug-ingredient\n",
            "Processing 77/101: https://www.fda.gov/drugs/medication-health-fraud/public-notification-sport-burner-contains-hidden-drug-ingredient\n",
            "Processing 78/101: https://www.fda.gov/drugs/medication-health-fraud/public-notification-adipessum-miracle-slimming-capsules-contain-hidden-drug-ingredient\n",
            "Processing 79/101: https://www.fda.gov/about-fda/center-drug-evaluation-and-research-cder/drug-safety-oversight-board-meeting-november-20-2014\n",
            "Processing 80/101: https://www.fda.gov/drugs/medication-health-fraud/public-notification-zi-xiu-tang-beauty-face-and-figure-capsule-contains-hidden-drug-ingredients\n",
            "Processing 81/101: https://www.fda.gov/drugs/medication-health-fraud/public-notification-sheaya-lender-contains-hidden-drug-ingredients\n",
            "Processing 82/101: https://www.fda.gov/drugs/medication-health-fraud/public-notification-nuvitra-contains-hidden-drug-ingredients\n",
            "Processing 83/101: https://www.fda.gov/drugs/medication-health-fraud/public-notification-livtone-contains-hidden-drug-ingredients\n",
            "Processing 84/101: https://www.fda.gov/animal-veterinary/product-safety-information/letters-veterinary-professionals\n",
            "Processing 85/101: https://www.fda.gov/news-events/press-announcements/fda-approves-first-medication-treat-severe-frostbite\n",
            "Processing 86/101: https://www.fda.gov/drugs/safe-use-over-counter-pain-relievers-and-fever-reducers/best-way-take-your-over-counter-pain-reliever-seriously-four-panel-brochure\n",
            "Processing 87/101: https://www.fda.gov/drugs/medication-health-fraud/public-notification-perfect-slim-fast-track-slim-contains-hidden-drug-ingredients\n",
            "Processing 88/101: https://www.fda.gov/drugs/medication-health-fraud/public-notification-slyn-both-contains-hidden-drug-ingredients\n",
            "Processing 89/101: https://www.fda.gov/drugs/medication-health-fraud/public-notification-zero-fat-contains-hidden-drug-ingredients\n",
            "Processing 90/101: https://www.fda.gov/drugs/medication-health-fraud/public-notification-queen-slimming-soft-gel-contains-hidden-drug-ingredient\n",
            "Processing 91/101: https://www.fda.gov/drugs/medication-health-fraud/public-notification-imperla-elita-vitaccino-contains-hidden-drug-ingredients\n",
            "Processing 92/101: https://www.fda.gov/drugs/medication-health-fraud/public-notification-dream-body-extreme-gold-contains-hidden-drug-ingredients\n",
            "Processing 93/101: https://www.fda.gov/drugs/fdas-labeling-resources-human-prescription-drugs/biological-products-specific-labeling-resources\n",
            "Processing 94/101: https://www.fda.gov/medical-devices/recently-approved-devices/minima-stent-system-p240003\n",
            "Processing 95/101: https://www.fda.gov/drugs/drug-safety-and-availability/2016-drug-safety-communications\n",
            "Processing 96/101: https://www.fda.gov/drugs/medication-health-fraud/public-notification-natural-max-slimming-contains-hidden-drug-ingredients\n",
            "Processing 97/101: https://www.fda.gov/drugs/medication-health-fraud/public-notification-dream-body-advanced-acai-weight-loss-cleanse-contains-hidden-drug-ingredients\n",
            "Processing 98/101: https://www.fda.gov/consumers/free-publications-women/sepa-como-tomar-sus-medicamentos-use-medicines-wisely\n",
            "Processing 99/101: https://www.fda.gov/consumers/womens-health-topics/women-and-pain-medicines\n",
            "Processing 100/101: https://www.fda.gov/consumers/free-publications-women/mis-medicinas\n",
            "Processing 101/101: https://www.fda.gov/consumers/consumer-updates/treating-migraines-ways-fight-pain-medication\n",
            "Web + File scraping completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "df = pd.read_csv(\"aspirin_scraped_texts.csv\")\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "ae_lexicon = [\n",
        "    \"abdominal pain\", \"acid reflux\", \"allergic reaction\", \"anaemia\", \"anxiety\",\n",
        "    \"asthma\", \"back pain\", \"bleeding\", \"blood in stool\", \"bruising\", \"chest pain\",\n",
        "    \"confusion\", \"constipation\", \"cough\", \"diarrhoea\", \"dizziness\", \"drowsiness\",\n",
        "    \"dry mouth\", \"dyspnoea\", \"ear ringing\", \"eczema\", \"fatigue\", \"fever\",\n",
        "    \"flatulence\", \"flushing\", \"gastritis\", \"gastrointestinal bleeding\",\n",
        "    \"hair loss\", \"headache\", \"hearing loss\", \"heartburn\", \"hives\", \"hypertension\",\n",
        "    \"indigestion\", \"internal bleeding\", \"itching\", \"joint pain\", \"kidney failure\",\n",
        "    \"light-headedness\", \"liver toxicity\", \"loss of appetite\", \"mouth ulcers\",\n",
        "    \"muscle pain\", \"nausea\", \"nosebleed\", \"palpitations\", \"rash\", \"renal failure\",\n",
        "    \"seizure\", \"shortness of breath\", \"skin irritation\", \"sleepiness\",\n",
        "    \"stomach pain\", \"sweating\", \"ulcer\", \"upper GI bleeding\", \"urine discolouration\",\n",
        "    \"vertigo\", \"vomiting\", \"weakness\"\n",
        "]\n",
        "\n",
        "aspirin_brand_names = [\"Bayer\", \"Bufferin\", \"Ecotrin\", \"Ascriptin\", \"Anacin\", \"Excedrin\", \"Disprin\", \"Alka-Seltzer\",\n",
        "    \"Aspilet\", \"Cardiprin\", \"CVS Health Aspirin\", \"Walgreens Aspirin\", \"Kirkland Signature Aspirin\",\n",
        "    \"Rite Aid Aspirin\", \"Equate Aspirin\", \"St. Joseph Low Dose Aspirin\", \"Nu-Seals Aspirin\", \"Caprin\",\n",
        "    \"Boots Aspirin\", \"Ecosprin\", \"ASA\", \"Aspirin IP\", \"Clopivas-AP\", \"Cartia\", \"Astrix\", \"Solprin\",\n",
        "    \"Herron Aspirin\", \"Halfprin\", \"Durlaza\", \"Fasprin\", \"Aspocid\", \"Disprin CV\", \"Aspir-low\",\n",
        "    \"Aspirin Protect\", \"Aspir 81\", \"ASA EC\", \"Aspro Clear\", \"Aspro\", \"Aspirina Bayer\", \"Micropirin\",\n",
        "    \"HeartSure Aspirin\", \"Acetylsalicylic Acid Tablets\", \"Cartia XT\", \"Asasantin\", \"Heron Low Dose Aspirin\",\n",
        "    \"Aspro Clear Extra\", \"Aspirin Cardio\", \"Aspilet 81 mg\", \"Aspenter\", \"Thrombo ASS\", \"Coraspin\",\n",
        "    \"Aspirin Protect 100\", \"Aspirin Dispersible IP\", \"Aspirin BP\", \"Aspocid 75\", \"Aspicot\", \"Ecosprin AV\",\n",
        "    \"Clopitab A\", \"Aspin\", \"Enteric Coated Aspirin\", \"Low Dose Aspirin 81 mg\", \"Baby Aspirin\", \"ASA Tablets\"\n",
        "]\n",
        "\n",
        "def detect_brand(sentence, brands):\n",
        "    for brand in brands:\n",
        "        if brand.lower() in sentence.lower():\n",
        "            return brand\n",
        "    return \"Generic\"\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "for ae in ae_lexicon:\n",
        "    pattern = [{\"LOWER\": token.lower()} for token in ae.split()]\n",
        "    matcher.add(ae, [pattern])\n",
        "\n",
        "results = []\n",
        "for idx, row in df.iterrows():\n",
        "    doc = nlp(str(row[\"Text\"]))\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for match_id, start, end in matches:\n",
        "        span = doc[start:end]\n",
        "        sentence = span.sent\n",
        "        ae_term = span.text.strip()\n",
        "        brand = detect_brand(sentence.text, aspirin_brand_names)\n",
        "\n",
        "        results.append({\n",
        "            \"URL\": row[\"URL\"],\n",
        "            \"Sentence\": sentence.text.strip(),\n",
        "            \"AE_Term\": ae_term,\n",
        "            \"Brand_Name\": brand,\n",
        "            \"Rule\": \"Lexicon Pattern Match\"\n",
        "        })\n",
        "\n",
        "output_df = pd.DataFrame(results)\n",
        "output_df.to_csv(\"aspirin_ae_with_brands.csv\", index=False)\n",
        "\n",
        "print(\"AE extraction with brand names complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7G2cdndiWo8B",
        "outputId": "1bbd47b6-d144-4987-ffaf-d733372c9bd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AE extraction with brand names complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the AE DataFrame\n",
        "ae_df = pd.read_csv(\"/content/aspirin_ae_with_brands.csv\")\n",
        "\n",
        "# List of positive keywords-manual filtering\n",
        "positive_keywords = [\n",
        "    \"relief\", \"relieved\", \"improved\", \"improvement\", \"better\", \"effective\", \"effectively\",\n",
        "    \"alleviated\", \"alleviation\", \"reduced\", \"reduction\", \"controlled\", \"resolved\", \"stable\",\n",
        "    \"managed\", \"beneficial\", \"helped\", \"helpful\", \"successfully\", \"positive response\",\n",
        "    \"pain-free\", \"tolerated\", \"no adverse event\", \"well tolerated\", \"responded well\",\n",
        "    \"good outcome\", \"no issues\", \"no side effects\", \"no symptoms\", \"enhanced\", \"less pain\",\n",
        "    \"comfort\", \"safe\", \"safety\", \"no problems\", \"symptom-free\", \"well-being\", \"cured\",\n",
        "    \"recover\", \"recovered\", \"normal\", \"without incident\", \"healed\", \"immunity\", \"boosted\",\n",
        "    \"prevented\", \"preventive\", \"ameliorated\", \"reassuring\", \"no complications\",\n",
        "    \"no concerns\", \"non-serious\", \"resolved spontaneously\", \"subsided\", \"tolerable\"\n",
        "]\n",
        "\n",
        "#Only keep rows where none of the positive keywords appear in the sentence\n",
        "def is_negative_ae(sentence):\n",
        "    sentence_lower = str(sentence).lower()\n",
        "    return not any(pos_kw in sentence_lower for pos_kw in positive_keywords)\n",
        "\n",
        "negative_ae_df = ae_df[ae_df[\"Sentence\"].apply(is_negative_ae)]\n",
        "\n",
        "# Save to CSV\n",
        "negative_ae_path = \"aspirin_negative_ae_only.csv\"\n",
        "negative_ae_df.to_csv(negative_ae_path, index=False)\n",
        "\n",
        "negative_ae_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "hXiX3w3WYAYS",
        "outputId": "c0b8d871-b83a-4166-be35-eb1ed4fa142a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'aspirin_negative_ae_only.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Load annotated AE file\n",
        "ae_df = pd.read_csv(\"aspirin_ae_with_brands.csv\")  # Rename this as needed\n",
        "\n",
        "#Positive keywords filter\n",
        "positive_keywords = [\n",
        "    \"relief\", \"relieved\", \"improved\", \"improvement\", \"better\", \"effective\", \"effectively\",\n",
        "    \"alleviated\", \"alleviation\", \"reduced\", \"reduction\", \"controlled\", \"resolved\", \"stable\",\n",
        "    \"managed\", \"beneficial\", \"helped\", \"helpful\", \"successfully\", \"positive response\",\n",
        "    \"pain-free\", \"tolerated\", \"no adverse event\", \"well tolerated\", \"responded well\",\n",
        "    \"good outcome\", \"no issues\", \"no side effects\", \"no symptoms\", \"enhanced\", \"less pain\",\n",
        "    \"comfort\", \"safe\", \"safety\", \"no problems\", \"symptom-free\", \"well-being\", \"cured\",\n",
        "    \"recover\", \"recovered\", \"normal\", \"without incident\", \"healed\", \"immunity\", \"boosted\",\n",
        "    \"prevented\", \"preventive\", \"ameliorated\", \"reassuring\", \"no complications\",\n",
        "    \"no concerns\", \"non-serious\", \"resolved spontaneously\", \"subsided\", \"tolerable\"\n",
        "]\n",
        "\n",
        "# Remove positive sentences\n",
        "def is_potentially_negative(sentence):\n",
        "    sentence_lower = str(sentence).lower()\n",
        "    return not any(keyword in sentence_lower for keyword in positive_keywords)\n",
        "\n",
        "keyword_filtered_df = ae_df[ae_df[\"Sentence\"].apply(is_potentially_negative)]\n",
        "\n",
        "# Sentiment analysis to keep only negative-toned sentences\n",
        "def is_negative_sentiment(sentence):\n",
        "    try:\n",
        "        polarity = TextBlob(str(sentence)).sentiment.polarity\n",
        "        return polarity < 0\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "final_filtered_df = keyword_filtered_df[keyword_filtered_df[\"Sentence\"].apply(is_negative_sentiment)]\n",
        "\n",
        "# Save to CSV\n",
        "final_filtered_df.to_csv(\"aspirin_negative_ae_final.csv\", index=False)\n",
        "print(\"Final file saved as 'aspirin_negative_ae_final.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjUi2ebCZNIY",
        "outputId": "c4a3a6e5-086e-474e-8379-bb48bd5723ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final file saved as 'aspirin_negative_ae_final.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from urllib.parse import quote\n",
        "\n",
        "# Load filtered AE file\n",
        "df = pd.read_csv(\"aspirin_negative_ae_final.csv\")\n",
        "\n",
        "# Setup BIOPortal\n",
        "API_KEY = \"6b9bc62c-283d-4ec5-805c-60f926e45feb\"\n",
        "BIOPORTAL_URL = \"http://data.bioontology.org/search?q={}&require_exact_match=true&apikey={}\"\n",
        "\n",
        "# Get unique AE terms\n",
        "unique_terms = df['AE_Term'].dropna().unique()\n",
        "\n",
        "# Map each AE term using BioPortal\n",
        "results = []\n",
        "for term in unique_terms:\n",
        "    encoded_term = quote(term)\n",
        "    url = BIOPORTAL_URL.format(encoded_term, API_KEY)\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            if data['collection']:\n",
        "                best_match = data['collection'][0]\n",
        "                results.append({\n",
        "                    \"AE_Term\": term,\n",
        "                    \"Mapped_ID\": best_match.get('@id', ''),\n",
        "                    \"Preferred_Label\": best_match.get('prefLabel', ''),\n",
        "                    \"Ontology\": best_match.get('links', {}).get('ontology', '').split('/')[-1]\n",
        "                })\n",
        "            else:\n",
        "                results.append({\n",
        "                    \"AE_Term\": term,\n",
        "                    \"Mapped_ID\": '',\n",
        "                    \"Preferred_Label\": '',\n",
        "                    \"Ontology\": ''\n",
        "                })\n",
        "        else:\n",
        "            print(f\"HTTP error for {term}: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Exception for {term}: {e}\")\n",
        "        results.append({\n",
        "            \"AE_Term\": term,\n",
        "            \"Mapped_ID\": '',\n",
        "            \"Preferred_Label\": '',\n",
        "            \"Ontology\": ''\n",
        "        })\n",
        "\n",
        "# Merge mapped results back into the filtered AE dataset\n",
        "mapped_df = pd.DataFrame(results)\n",
        "final_mapped_df = pd.merge(df, mapped_df, on=\"AE_Term\", how=\"left\")\n",
        "\n",
        "# Save output\n",
        "output_path = \"aspirin_negative_ae_final_mapped.csv\"\n",
        "final_mapped_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Ontology mapping complete. Saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uM6xyarsaYAb",
        "outputId": "0ac1eed3-54a2-47ac-845e-3bf1992ba5b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ontology mapping complete. Saved to aspirin_negative_ae_final_mapped.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FaceBook model for AE filtering\n",
        "INPUT_FILE = \"/content/aspirin_ae_with_brands.csv\"\n",
        "OUTPUT_FILE = \"/content/aspirin_ae_clinicalbert_zero_shot_final.csv\"\n",
        "\n",
        "df = pd.read_csv(INPUT_FILE)\n",
        "\n",
        "required_cols = [\"Sentence\", \"AE_Term\", \"Brand_Name\", \"URL\"]\n",
        "for col in required_cols:\n",
        "    if col not in df.columns:\n",
        "        raise ValueError(f\"Missing column: {col}\")\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "candidate_labels = [\"adverse event\", \"normal statement\", \"beneficial effect\"]\n",
        "\n",
        "results = []\n",
        "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    sentence = row[\"Sentence\"]\n",
        "    ae_term = row[\"AE_Term\"]\n",
        "    brand = row[\"Brand_Name\"]\n",
        "    url = row[\"URL\"]\n",
        "\n",
        "    if not isinstance(sentence, str) or sentence.strip() == \"\":\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        output = classifier(sentence, candidate_labels)\n",
        "        prediction_label = output[\"labels\"][0]\n",
        "        confidence_score = round(output[\"scores\"][0], 4)\n",
        "\n",
        "        results.append({\n",
        "            \"Sentence\": sentence,\n",
        "            \"AE Term\": ae_term,\n",
        "            \"Brand\": brand,\n",
        "            \"Prediction Label\": prediction_label,\n",
        "            \"Confidence Score\": confidence_score,\n",
        "            \"Paper Link\": url\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing sentence: {e}\")\n",
        "        continue\n",
        "\n",
        "df_out = pd.DataFrame(results)\n",
        "\n",
        "df_out.to_csv(OUTPUT_FILE, index=False)\n",
        "print(f\"Classification complete. File saved to: {OUTPUT_FILE}\")"
      ],
      "metadata": {
        "id": "JvByxGkkZiss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "\n",
        "@st.cache_data\n",
        "def load_data():\n",
        "    return pd.read_csv(\"aspirin_negative_ae_final_mapped.csv\")\n",
        "\n",
        "df = load_data()\n",
        "\n",
        "st.set_page_config(page_title=\"AE Viewer\", layout=\"wide\")\n",
        "st.title(\"Aspirin Adverse Events Ontology Mapping\")\n",
        "\n",
        "# Sidebar Filters\n",
        "with st.sidebar:\n",
        "    st.header(\"Filter Options\")\n",
        "    ae_terms = st.multiselect(\"Filter by AE Term\", sorted(df[\"AE_Term\"].dropna().unique()))\n",
        "    brands = st.multiselect(\"Filter by Brand Name\", sorted(df[\"Brand_Name\"].dropna().unique()))\n",
        "    ontologies = st.multiselect(\"Filter by Ontology\", sorted(df[\"Ontology\"].dropna().unique()))\n",
        "\n",
        "    filtered_df = df.copy()\n",
        "    if ae_terms:\n",
        "        filtered_df = filtered_df[filtered_df[\"AE_Term\"].isin(ae_terms)]\n",
        "    if brands:\n",
        "        filtered_df = filtered_df[filtered_df[\"Brand_Name\"].isin(brands)]\n",
        "    if ontologies:\n",
        "        filtered_df = filtered_df[filtered_df[\"Ontology\"].isin(ontologies)]\n",
        "\n",
        "# Display Results\n",
        "st.markdown(f\"### Showing {len(filtered_df)} Records\")\n",
        "st.dataframe(filtered_df, use_container_width=True)\n",
        "\n",
        "# Summary Section\n",
        "st.markdown(\"### Summary\")\n",
        "col1, col2, col3 = st.columns(3)\n",
        "col1.metric(\"Unique AE Terms\", df[\"AE_Term\"].nunique())\n",
        "col2.metric(\"Mapped Ontologies\", df[\"Ontology\"].nunique())\n",
        "col3.metric(\"Total Records\", len(df))\n",
        "\n",
        "# File Download\n",
        "st.markdown(\"### Download\")\n",
        "st.download_button(\n",
        "    label=\"Download Full CSV\",\n",
        "    data=df.to_csv(index=False).encode(\"utf-8\"),\n",
        "    file_name=\"aspirin_negative_ae_final_mapped.csv\",\n",
        "    mime=\"text/csv\"\n",
        ")\n",
        "\n",
        "st.download_button(\n",
        "    label=\"Download Filtered CSV\",\n",
        "    data=filtered_df.to_csv(index=False).encode(\"utf-8\"),\n",
        "    file_name=\"filtered_aspirin_ae.csv\",\n",
        "    mime=\"text/csv\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEEWLiK7NcxK",
        "outputId": "5ac44ad9-b445-4755-f04f-96c94cbbaa10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "F_-6Poa6NdEr",
        "outputId": "36c14dfb-af62-4a81-be77-1b8081416b59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4fc35ef3-780d-41b1-8204-da20c7967e6f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4fc35ef3-780d-41b1-8204-da20c7967e6f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving aspirin_negative_ae_final_mapped.csv to aspirin_negative_ae_final_mapped (1).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok --quiet"
      ],
      "metadata": {
        "id": "wHD_omNnhTnq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d396290b-6e73-4fb7-c455-92c2d2742c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ngork to launch the UI\n",
        "from pyngrok import ngrok\n",
        "import time\n",
        "ngrok.set_auth_token(\"2zjUnOGKNzBXtqR9A0lPIFRAdH5_fRSU4vT3LWBugMkACPLe\")\n",
        "\n",
        "!pkill streamlit\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Public URL:\", public_url)\n",
        "!streamlit run app.py &>/content/log.txt &\n",
        "time.sleep(10)\n",
        "print(\"App is now running!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cyjEChBhjbY",
        "outputId": "fbd715d7-be7c-4871-b444-4551b0200daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://1842c14cce33.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "App is now running!\n"
          ]
        }
      ]
    }
  ]
}